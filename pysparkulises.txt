#!/usr/bin/env python3

import pandas as pd
from pyspark.sql.types import *
import databricks.koalas as ks
from datetime import datetime
import collections as cl
import datetime
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
from pyspark import SparkContext
from pyspark.sql import SQLContext #Libraries I used

#Opening files
weather1 = pd.read_csv("weather01.txt",delim_whitespace = True)
weather2 = pd.read_csv("weather02.txt",delim_whitespace = True)
weather3 = pd.read_csv("weather03.txt",delim_whitespace = True)
weather4 = pd.read_csv("weather04.txt",delim_whitespace = True)
weather5 = pd.read_csv("weather05.txt",delim_whitespace = True)
weather6 = pd.read_csv("weather06.txt",delim_whitespace = True)

#inspecting all the files
weather1.head()
weather2.head()
weather3.head()
weather4.head()
weather5.head()
weather6.head()

#Joining them
weather = pd.concat([weather1,weather2,weather3,weather4,weather5,weather6], axis=1)

#Deleting X
del(weather['X'])
#Melting
weather = pd.melt(weather,
                 id_vars=['year', 'month', 'measure'],
                 var_name='day');
#Droping non values
weather.dropna(inplace=True)
#Reseting index
weather = weather.reset_index(drop=True)

#Renaming columns
weather=weather.rename({'year': 'Year', 'month': 'Month','measure': 'Weather','day':'Day','value':'Value'})


#Using function to dating correctly
def creating_date(row):
    return "%d-%02d-%02d" % (row['year'], row['month'], int(row['day'][1:]))

#Applying function
weather['Date'] = weather.apply(creating_date,axis=1)
#Selecting columns
weather = weather [['measure', 'value', 'Date']]

#Droping missing values
weather = weather.replace('<NA>',np.NaN)
weather = weather.dropna()

#Using pivot
weather = weather.pivot(index='Date', columns= 'measure', values='value')
#Reseting index
weather = weather.reset_index()
#Using only the 365 days
weather = weather.drop(365,axis=0)
#Choosing random values instead of having non values
weather['Events'] =weather['Events'].fillna(method='ffill')

#Sorting values
weather = weather.sort_values('Date',ascending= False)
#Indexing date
weather = weather.set_index('Date',drop=True)
weather.index = pd.to_datetime(weather.index)

#Validating data
weather.info()

#Changing data types
weather['Max.TemperatureF'] = weather['Max.TemperatureF'].astype(int)
weather['Min.TemperatureF'] = weather['Min.TemperatureF'].astype(int)
weather['Mean.TemperatureF'] = weather['Mean.TemperatureF'].astype(int)
weather['Max.Dew.PointF'] = weather['Max.Dew.PointF'].astype(int)
weather['MeanDew.PointF'] = weather['MeanDew.PointF'].astype(int)
weather['Min.DewpointF'] = weather['Min.DewpointF'].astype(int)

#Transforming it to koalas
koalaweather = ks.from_pandas(weather)

#Transforming it to csv 
weather.to_csv()

#Putting it in a csv file
weather.to_csv('koalaweather.csv')

#Stopping sc pyspark workflow

sc.stop()